{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# utils/llm.py\n",
    "\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\1-06\\Desktop\\team4-backend\\venv\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "    넌 질문-답변을 도와주는 AI 영화 추천기야.\n",
    "    아래 제공되는 Context를 통해서 사용자 Question에 대해 답을 해줘야해.\n",
    "\n",
    "    Context에는 직접적으로 없어도, 추론하거나 계산할 수 있는 답변은 최대한 만들어 봐.\n",
    "    만약 우리가 제공한 csv 파일에 관련 영화가 존재하지 않는다면, \"죄송합니다. 관련 영화가 없습니다. 다른 영화를 추천 받으시겠어요?\"라는 답변을 생성해줘.\n",
    "\n",
    "    답은 적절히 \\n를 통해 문단을 나눠줘 한국어로 만들어 줘. \n",
    "    # Question:\n",
    "    {question}\n",
    "\n",
    "    # Context:\n",
    "    {context}\n",
    "\n",
    "\n",
    "    # Answer:\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm(user_input):\n",
    "    vectorstore = PineconeVectorStore.from_existing_index(\n",
    "        index_name = os.environ.get('INDEX_NAME'),\n",
    "        embedding=OpenAIEmbeddings()\n",
    "    )\n",
    "    \n",
    "    # 5. Retrieve\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    # 6. Prompting\n",
    "    prompt = PromptTemplate.from_template(text)\n",
    "\n",
    "    # 7. LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    parser = StrOutputParser()\n",
    "\n",
    "    # 8. Chain\n",
    "    chain = (\n",
    "        {'context': retriever, 'question': RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | parser\n",
    "    )\n",
    "\n",
    "    ans = chain.invoke(user_input)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
